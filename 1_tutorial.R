
library(tidyverse)

# ------------------------------------------------
# - Работа с PDF ---------------------------------
# ------------------------------------------------

# 1. Скачали пдфы с сайта
# 2. Склеили в 1 пдф (https://www.ilovepdf.com/ru)
# 3. Перевели пдф в ворд
# 4. Из ворд скопировали текст в txt

# Загружаем текст
t <- readLines('data/nizhniy_novgorod.txt', encoding = 'UTF-8')

# Все склеиваем
t2 <- paste(t, collapse = ' ')

# Разделяем единый кусок текста на части по слову-разделителю "Заключение"
t3 <- strsplit(t2, split = 'Заключение')

# Вытащим текстовый вектор из листа
t4 <- t3[[1]]

# Оставим только заключения (избавимся от 1-го мусорного объекта)
t5 <- t4[2:4]

# Создаем таблицу, где строчки - куски текста из каждого заключения
df <- data.frame(text = t5)

# Что хотим вытащить:
# 1. Дата начала ПС
# 2. Дата завершения ПС
# 3. Есть ли возражения/предложения

# ctrl + shift + m - быстрый вызов символа %>%

df2 <- df %>%
  mutate(
    # Вытищили дату через регулярные выражения
    date_start = str_replace_all(string = text, pattern = '.*Экспозиция проекта проводилась: с ([0-9]{2}\\.[0-9]{2}\\.[0-9]{4}).*', replacement = '\\1'),
    # Посчитали кол-во раз, сколько встречается в тексте паттерн
    n = str_count(string = text, pattern = 'Не поступало|Не поступали'), # | - знак условия "или"
    # Определили, состоялись ли слушания
    is_ok = !(str_detect(string = text, pattern = '.*не[:blank:]*состоявшимися.*'))
  )

# Какие бывают проблемы при работе с текстом:
# 1. Пробел - не всегда пробел

# Сохраняем таблицу, чтоб работать дальше в excel
library(openxlsx)

write.xlsx(df2, 'data/1_excel.xlsx')

# ------------------------------------------------
# - Работа с текстом, размещенным на сайте -------
# ------------------------------------------------

library(rvest)
library(httr)

page <- 'https://admgor.nnov.ru/Gorod/Napravleniya-raboty/Gradostroitelstvo/Publichnye-slushaniya/Publichnye-slushaniya-2020'

urls <- page %>% 
  # считываем структуру сайта
  read_html() %>% 
  # находим все элементы с тегом <a></a>
  html_nodes('a') %>% 
  # вытаскиываем аттрибут href, который прописан внутри тега - hyper reference (гиперссылка)
  html_attr('href')

# создаем таблицу, с ссылками
df_urls <- data.frame(url = urls) %>% 
  mutate(
    # добавляем недостающую часть к полученным ссылкам
    url = str_c('https://admgor.nnov.ru', url),
    # определяем, ссылка ведет на заключение?
    zakluchenie = str_detect(string = tolower(url), pattern = 'заключение'),
    # определяем, ссылка ведет на оповещение?
    opoveshenie = str_detect(string = url, pattern = 'getODA')
  )

# создаем новую таблицу только с оповещениями
df_opovesheniya <- df_urls %>% 
  filter(opoveshenie == TRUE)

# создадим функцию, которая принимает на вход ссылку, а выдает текст по этой ссылке
read_url <- function(url) {
  
  text <- url %>% 
    # считываем структуру страницы
    read_html() %>% 
    # вытаскиваем весь текст со страницы
    html_text()
  
  # для отслеживания прогресса выводим в консоль ссылку
  print(url)
  return(text)
}

# функция map_chr проходит по каждому объекту в аргументе .x и применяет к ним функцию .f
df_opovesheniya$text <- map_chr(.x = df_opovesheniya$url, .f = read_url)

